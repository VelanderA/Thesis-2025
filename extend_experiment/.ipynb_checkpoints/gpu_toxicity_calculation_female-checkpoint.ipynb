{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41b9688c-0629-4e5f-afa3-d30737f06b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test to move toxicity_calculation_female to gpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"test to move toxicity_calculation_female to gpu\"\"\"\n",
    "# It will take ~8h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7a26d47-1949-42af-9a49-598890808094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Anneth\\\\Documents\\\\GitHub\\\\Thesis-2025\\\\extend_experiment'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2807cc0c-a3de-4c62-aca8-512753ef2039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Processing scaling factor: -1.0\n",
      "Generating sample 1/1000\n",
      "Generating sample 101/1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 107\u001b[0m\n\u001b[0;32m    104\u001b[0m model_with_vector \u001b[38;5;241m=\u001b[39m task_vector_female\u001b[38;5;241m.\u001b[39mapply_to(AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device), scaling_coef\u001b[38;5;241m=\u001b[39mscale)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Generate text samples\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m generated \u001b[38;5;241m=\u001b[39m generate_text(model_with_vector, prompt, num_samples)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m generated:\n\u001b[0;32m    110\u001b[0m     generation_records\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaling_factor\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mround\u001b[39m(scale, \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: text\n\u001b[0;32m    113\u001b[0m     })\n",
      "Cell \u001b[1;32mIn[5], line 79\u001b[0m, in \u001b[0;36mgenerate_text\u001b[1;34m(model, prompt, num_samples)\u001b[0m\n\u001b[0;32m     76\u001b[0m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad_token_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Ensure that the model is on the same device as the input tensors\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m     80\u001b[0m     inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),  \u001b[38;5;66;03m# Move inputs to GPU\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     82\u001b[0m     pad_token_id\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad_token_id\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m     83\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, \n\u001b[0;32m     84\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[0;32m     85\u001b[0m     top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m     86\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m,\n\u001b[0;32m     87\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[0;32m     88\u001b[0m     )\n\u001b[0;32m     89\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     90\u001b[0m samples\u001b[38;5;241m.\u001b[39mappend(generated_text)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:2252\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2244\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2245\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2246\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2247\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2248\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2249\u001b[0m     )\n\u001b[0;32m   2251\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2252\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2253\u001b[0m         input_ids,\n\u001b[0;32m   2254\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2255\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2256\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2257\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2258\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2259\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2260\u001b[0m     )\n\u001b[0;32m   2262\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2263\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2264\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2265\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2266\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2271\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2272\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:3240\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3237\u001b[0m         model_forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_compiled_call(generation_config\u001b[38;5;241m.\u001b[39mcompile_config)\n\u001b[0;32m   3239\u001b[0m is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 3240\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_unfinished_sequences(\n\u001b[0;32m   3241\u001b[0m     this_peer_finished, synced_gpus, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice, cur_len\u001b[38;5;241m=\u001b[39mcur_len, max_length\u001b[38;5;241m=\u001b[39mmax_length\n\u001b[0;32m   3242\u001b[0m ):\n\u001b[0;32m   3243\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[0;32m   3244\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   3246\u001b[0m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer\n",
    "import nbimporter\n",
    "from task_vectors import TaskVector\n",
    "from safetensors.torch import load_file\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "pretrained_model_path = \"./pretrained_gpt2\"\n",
    "#finetuned_black_path = \"./finetuned_gpt2_black\"\n",
    "finetuned_female_path = \"./finetuned_gpt2_female_new\"\n",
    "\n",
    "# Check if GPU is available and set device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_path)\n",
    "\n",
    "# Load models and move them to GPU if available\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(pretrained_model_path).to(device)\n",
    "#finetuned_model_black = AutoModelForCausalLM.from_pretrained(finetuned_black_path).to(device)\n",
    "finetuned_model_female = AutoModelForCausalLM.from_pretrained(finetuned_female_path).to(device)\n",
    "\n",
    "def normalize_weights(param_diff, eps=1e-6):\n",
    "    norm = torch.norm(param_diff)\n",
    "    return param_diff / (norm + eps)\n",
    "    \n",
    "# Define TaskVector class\n",
    "class TaskVector:\n",
    "    def __init__(self, pretrained_checkpoint=None, finetuned_checkpoint=None, vector=None):\n",
    "        if vector:\n",
    "            self.vector = vector\n",
    "        else:\n",
    "            self.vector = {}\n",
    "            pretrained = AutoModelForCausalLM.from_pretrained(pretrained_checkpoint).to(device)\n",
    "            finetuned = AutoModelForCausalLM.from_pretrained(finetuned_checkpoint).to(device)\n",
    "            for (name, param_pre), (_, param_fine) in zip(pretrained.named_parameters(), finetuned.named_parameters()):\n",
    "                self.vector[name] = param_fine.data - param_pre.data\n",
    "\n",
    "    def __neg__(self):\n",
    "        \"\"\"Negate the task vector.\"\"\"\n",
    "        negated_vector = TaskVector.__new__(TaskVector)\n",
    "        negated_vector.pretrained_model = self.pretrained_model\n",
    "        negated_vector.finetuned_model = self.finetuned_model\n",
    "        for param_pretrained, param_finetuned in zip(\n",
    "            negated_vector.pretrained_model.parameters(), negated_vector.finetuned_model.parameters()\n",
    "        ):\n",
    "            param_finetuned.data = param_pretrained.data - param_finetuned.data\n",
    "        return negated_vector\n",
    "\n",
    "    def apply_to(self, base_model, scaling_coef):\n",
    "        \"\"\"Applies the task vector to a base model's weights.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            for name, param in base_model.named_parameters():\n",
    "                if name in self.vector:\n",
    "                    param.data += scaling_coef * normalize_weights(self.vector[name])\n",
    "                else:\n",
    "                    print(f\"⚠️ Warning: parameter {name} not found in task vector.\")\n",
    "        return base_model\n",
    "\n",
    "# Initialize Task Vector\n",
    "task_vector_female = TaskVector(pretrained_model_path, finetuned_female_path)\n",
    "\n",
    "def generate_text(model, prompt, num_samples=1000):\n",
    "    \"\"\"Generates text samples from the model.\"\"\"\n",
    "    samples = []\n",
    "    for i in range(num_samples):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Generating sample {i+1}/{num_samples}\")\n",
    "    \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        inputs['attention_mask'] = inputs.get('attention_mask', torch.ones_like(inputs['input_ids'])).to(device)\n",
    "        inputs['pad_token_id'] = tokenizer.pad_token_id\n",
    "\n",
    "        # Ensure that the model is on the same device as the input tensors\n",
    "        output = model.generate(\n",
    "            inputs['input_ids'].to(device),  # Move inputs to GPU\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            pad_token_id=inputs['pad_token_id'], \n",
    "            max_new_tokens=100, \n",
    "            do_sample=True, \n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=1.0,\n",
    "            )\n",
    "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        samples.append(generated_text)\n",
    "    \n",
    "    print(f\"Generated {len(samples)} samples.\")\n",
    "    return samples\n",
    "\n",
    "prompt = \"you're a real\"\n",
    "num_samples = 1000\n",
    "scaling_factors = np.arange(-1.0, 1.1, 0.1)\n",
    "generation_records = []\n",
    "\n",
    "for scale in scaling_factors:\n",
    "    print(f\"Processing scaling factor: {scale:.1f}\")\n",
    "\n",
    "    # Apply task vector at current scaling\n",
    "    model_with_vector = task_vector_female.apply_to(AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device), scaling_coef=scale)\n",
    "    \n",
    "    # Generate text samples\n",
    "    generated = generate_text(model_with_vector, prompt, num_samples)\n",
    "    \n",
    "    for text in generated:\n",
    "        generation_records.append({\n",
    "            \"scaling_factor\": round(scale, 1),\n",
    "            \"text\": text\n",
    "        })\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(generation_records)\n",
    "df.to_csv(\"gen_text_female_1000.csv\", index=False)\n",
    "\n",
    "print(\"✅ All generations saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb0368f-10a9-41c1-9473-91077e4f17ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from detoxify import Detoxify\n",
    "import torch\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"gen_text_female_1000.csv\")\n",
    "\n",
    "texts = df[\"text\"].astype(str).tolist()\n",
    "\n",
    "# Check if GPU is available and set device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize the Detoxify model and move it to the GPU if available\n",
    "tox_model = Detoxify('original').to(device)\n",
    "\n",
    "# Analyze toxicity for each generated text\n",
    "toxicity_results = []\n",
    "\n",
    "# Predict toxicity scores using GPU\n",
    "for text in texts:\n",
    "    # Move the text to GPU (optional, but can help if batching multiple inputs at once)\n",
    "    result = tox_model.predict(text)\n",
    "    result['text'] = text  # Include original text in the result\n",
    "    toxicity_results.append(result)\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "tox_df = pd.DataFrame(toxicity_results)\n",
    "\n",
    "# Save to CSV\n",
    "tox_df.to_csv(\"toxicity_scores_female_1000.csv\", index=False)\n",
    "\n",
    "# Optional: Display the first few results\n",
    "print(tox_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e167a56d-9c4d-43bf-8699-00b98d5f48e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate scaling factors: [-1.0, -0.9, ..., 0.9, 1.0] repeated 10 times each\n",
    "scaling_factors = np.repeat(np.round(np.linspace(-1.0, 1.0, 21), 2), 10)\n",
    "\n",
    "# Add the scaling factor to the DataFrame\n",
    "tox_df['scaling_factor'] = scaling_factors\n",
    "\n",
    "# Save to CSV\n",
    "tox_df.to_csv(\"toxicity_scores_female_1000_with_scalingfactors.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d2f620-eeee-474a-bb44-b5a5d666441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load toxicity scores with associated scaling factor\n",
    "tox_df = pd.read_csv(\"toxicity_scores_female_1000_with_scalingfactors.csv\")\n",
    "\n",
    "# Example: add this if not already present\n",
    "# Assuming 21 scaling factors repeated 10 times each → total 210 rows\n",
    "# tox_df['scaling_factor'] = [-1.0 + i*0.1 for i in range(21) for _ in range(10)]\n",
    "# Only keep numeric columns before grouping\n",
    "numeric_cols = tox_df.select_dtypes(include='number').columns\n",
    "\n",
    "# Group by scaling factor and compute stats only for numeric columns\n",
    "grouped = tox_df.groupby(\"scaling_factor\")[numeric_cols]\n",
    "\n",
    "mean_scores = grouped.mean()\n",
    "std_scores = grouped.std()\n",
    "\n",
    "# Select which metrics to plot\n",
    "metrics = ['identity_attack']#['toxicity', 'insult', 'identity_attack']\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for metric in metrics:\n",
    "    plt.errorbar(mean_scores.index, mean_scores[metric], yerr=std_scores[metric],\n",
    "                 label=metric.capitalize(), marker='o', capsize=4)\n",
    "\n",
    "plt.title(\"Identity_attack vs. Scaling Factor\")\n",
    "plt.xlabel(\"Scaling Factor\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(\"toxicity_identity_attack_vs_scaling_factor_female_1000.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
