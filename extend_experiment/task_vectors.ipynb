{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1e45afa-f98d-4c60-bbd7-906f012ad11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load the pretrained model\n",
    "pretrained_model = AutoModelForSequenceClassification.from_pretrained(\"albert-base-v2\", num_labels=2)\n",
    "\n",
    "# Paths to checkpoints\n",
    "pretrained_checkpoint = \"C:/Users/Anneth/TST/pretrained_albert/model.safetensors\"\n",
    "finetuned_checkpoint = \"C:/Users/Anneth/TST/fine_tuned_albert/model.safetensors\"\n",
    "\n",
    "class TaskVector():\n",
    "    def __init__(self, pretrained_checkpoint=None, finetuned_checkpoint=None, vector=None):\n",
    "        \"\"\"Initializes the task vector from a pretrained and a finetuned checkpoints.\n",
    "        \n",
    "        This can either be done by passing two state dicts (one corresponding to the\n",
    "        pretrained model, and another to the finetuned model), or by directly passying in\n",
    "        the task vector state dict.\n",
    "        \"\"\"\n",
    "        if vector is not None:\n",
    "            self.vector = vector\n",
    "        else:\n",
    "            assert pretrained_checkpoint is not None and finetuned_checkpoint is not None\n",
    "            with torch.no_grad():\n",
    "                #if pretrained_checkpoint.endswith(\".safetensors\"):\n",
    "                pretrained_state_dict = load_file(pretrained_checkpoint)\n",
    "                #else:\n",
    "                #    pretrained_state_dict = torch.load(pretrained_checkpoint).state_dict()\n",
    "            \n",
    "                #if finetuned_checkpoint.endswith(\".safetensors\"):\n",
    "                finetuned_state_dict = load_file(finetuned_checkpoint)\n",
    "                #else:\n",
    "                #    finetuned_state_dict = torch.load(finetuned_checkpoint).state_dict()\n",
    "                \n",
    "                self.vector = {}\n",
    "                for key in pretrained_state_dict:\n",
    "                    if pretrained_state_dict[key].dtype in [torch.int64, torch.uint8]:\n",
    "                        continue\n",
    "                    self.vector[key] = finetuned_state_dict[key] - pretrained_state_dict[key]\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        \"\"\"Add two task vectors together.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            new_vector = {}\n",
    "            for key in self.vector:\n",
    "                if key not in other.vector:\n",
    "                    print(f'Warning, key {key} is not present in both task vectors.')\n",
    "                    continue\n",
    "                new_vector[key] = self.vector[key] + other.vector[key]\n",
    "        return TaskVector(vector=new_vector)\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        if other is None or isinstance(other, int):\n",
    "            return self\n",
    "        return self.__add__(other)\n",
    "\n",
    "    def __neg__(self):\n",
    "        \"\"\"Negate a task vector.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            new_vector = {}\n",
    "            for key in self.vector:\n",
    "                new_vector[key] = - self.vector[key]\n",
    "        return TaskVector(vector=new_vector)\n",
    "\n",
    "    def apply_to(self, pretrained_model, scaling_coef=1.0): # changed \"pretrained_checkpoint\" to \"pretrained_model\"\n",
    "        \"\"\"Apply a task vector to a pretrained model.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            #if pretrained_checkpoint.endswith(\".safetensors\"):\n",
    "            #    pretrained_state_dict = load_file(pretrained_checkpoint)\n",
    "            #else:\n",
    "            pretrained_state_dict = pretrained_model.state_dict()\n",
    "            #    pretrained_state_dict = torch.load(pretrained_checkpoint).state_dict()\n",
    "            new_state_dict = {}\n",
    "            #pretrained_state_dict = pretrained_model.state_dict()\n",
    "            for key in pretrained_state_dict:\n",
    "                if key not in self.vector:\n",
    "                    print(f'Warning: key {key} is present in the pretrained state dict but not in the task vector')\n",
    "                    continue\n",
    "                new_state_dict[key] = pretrained_state_dict[key] + scaling_coef * self.vector[key]\n",
    "        pretrained_model.load_state_dict(new_state_dict, strict=False)\n",
    "        return pretrained_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27c8892-a090-4300-a9a8-2f6a9d2403fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
