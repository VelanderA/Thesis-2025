{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cb5f052-ee87-47ee-815a-945f05cef2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Anneth\\anaconda3\\envs\\torch\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"TASK VECTOR OPERATION\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer\n",
    "import nbimporter\n",
    "from task_vectors import TaskVector\n",
    "from safetensors.torch import load_file\n",
    "from transformers import pipeline\n",
    "from task_vectors import TaskVector\n",
    "from safetensors.torch import load_file\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "pretrained_model_path = \"./pretrained_gpt2\"\n",
    "finetuned_black_path = \"./finetuned_gpt2_black\"\n",
    "finetuned_female_path = \"./finetuned_gpt2_female\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_path)\n",
    "\n",
    "# Load models\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(pretrained_model_path)\n",
    "finetuned_model_black = AutoModelForCausalLM.from_pretrained(finetuned_black_path)\n",
    "finetuned_model_female = AutoModelForCausalLM.from_pretrained(finetuned_female_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dced5d59-041e-4402-8ab7-24823aaad342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scaling factor: -1.0\n",
      "Processing scaling factor: -0.9\n",
      "Processing scaling factor: -0.8\n",
      "Processing scaling factor: -0.7\n",
      "Processing scaling factor: -0.6\n",
      "Processing scaling factor: -0.5\n",
      "Processing scaling factor: -0.4\n",
      "Processing scaling factor: -0.3\n",
      "Processing scaling factor: -0.2\n",
      "Processing scaling factor: -0.1\n",
      "Processing scaling factor: -0.0\n",
      "Processing scaling factor: 0.1\n",
      "Processing scaling factor: 0.2\n",
      "Processing scaling factor: 0.3\n",
      "Processing scaling factor: 0.4\n",
      "Processing scaling factor: 0.5\n",
      "Processing scaling factor: 0.6\n",
      "Processing scaling factor: 0.7\n",
      "Processing scaling factor: 0.8\n",
      "Processing scaling factor: 0.9\n",
      "Processing scaling factor: 1.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 88\u001b[0m\n\u001b[0;32m     82\u001b[0m         generation_records\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     83\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaling_factor\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mround\u001b[39m(scale, \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     84\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: text\n\u001b[0;32m     85\u001b[0m         })\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Save to CSV\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(generation_records)\n\u001b[0;32m     89\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgen_text_black_10.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ All generations saved\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "'''check black vector'''\n",
    "#def normalize_weights(param_diff):\n",
    "#    return param_diff / torch.norm(param_diff)\n",
    "def normalize_weights(param_diff, eps=1e-6):\n",
    "    norm = torch.norm(param_diff)\n",
    "    return param_diff / (norm + eps)    \n",
    "# Define TaskVector class\n",
    "class TaskVector:\n",
    "    def __init__(self, pretrained_checkpoint=None, finetuned_checkpoint=None, vector=None):\n",
    "        if vector:\n",
    "            self.vector = vector\n",
    "        else:\n",
    "            self.vector = {}\n",
    "            pretrained = AutoModelForCausalLM.from_pretrained(pretrained_checkpoint)\n",
    "            finetuned = AutoModelForCausalLM.from_pretrained(finetuned_checkpoint)\n",
    "            for (name, param_pre), (_, param_fine) in zip(pretrained.named_parameters(), finetuned.named_parameters()):\n",
    "                self.vector[name] = param_fine.data - param_pre.data\n",
    "\n",
    "    def __neg__(self):\n",
    "        \"\"\"Negate the task vector.\"\"\"\n",
    "        negated_vector = TaskVector.__new__(TaskVector)\n",
    "        negated_vector.pretrained_model = self.pretrained_model\n",
    "        negated_vector.finetuned_model = self.finetuned_model\n",
    "        for param_pretrained, param_finetuned in zip(\n",
    "            negated_vector.pretrained_model.parameters(), negated_vector.finetuned_model.parameters()\n",
    "        ):\n",
    "            param_finetuned.data = param_pretrained.data - param_finetuned.data\n",
    "        return negated_vector\n",
    "\n",
    "    def apply_to(self, base_model, scaling_coef):\n",
    "        \"\"\"Applies the task vector to a base model's weights.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            for name, param in base_model.named_parameters():\n",
    "                if name in self.vector:\n",
    "                    param.data += scaling_coef * normalize_weights(self.vector[name])\n",
    "                    #param.data += scaling_coef * self.vector[name]\n",
    "                else:\n",
    "                    print(f\"⚠️ Warning: parameter {name} not found in task vector.\")\n",
    "        return base_model\n",
    "\n",
    "# Initialize Task Vector\n",
    "task_vector = TaskVector(pretrained_model_path, finetuned_black_path)\n",
    "\n",
    "def generate_text(model, prompt, num_samples=10):\n",
    "    \"\"\"Generates text samples from the model.\"\"\"\n",
    "    samples = []\n",
    "    for _ in range(num_samples):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        inputs['attention_mask'] = inputs.get('attention_mask', torch.ones_like(inputs['input_ids']))\n",
    "        inputs['pad_token_id'] = tokenizer.pad_token_id\n",
    "\n",
    "        output = model.generate(\n",
    "            inputs['input_ids'], \n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            pad_token_id=inputs['pad_token_id'], \n",
    "            max_new_tokens=100, \n",
    "            do_sample=True, \n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=1.0,\n",
    "            )\n",
    "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        samples.append(generated_text)\n",
    "\n",
    "    return samples\n",
    "\n",
    "prompt = \"you're a real\"\n",
    "num_samples = 10\n",
    "scaling_factors = np.arange(-1.0, 1.1, 0.1)\n",
    "generation_records = []\n",
    "\n",
    "for scale in scaling_factors:\n",
    "    print(f\"Processing scaling factor: {scale:.1f}\")\n",
    "    \n",
    "    # Apply task vector at current scaling\n",
    "    model_with_vector = task_vector.apply_to(AutoModelForCausalLM.from_pretrained(\"gpt2\"), scaling_coef=scale)\n",
    "    \n",
    "    # Generate text samples\n",
    "    generated = generate_text(model_with_vector, prompt, num_samples)\n",
    "    \n",
    "    for text in generated:\n",
    "        generation_records.append({\n",
    "            \"scaling_factor\": round(scale, 1),\n",
    "            \"text\": text\n",
    "        })\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(generation_records)\n",
    "df.to_csv(\"gen_text_black_10.csv\", index=False)\n",
    "\n",
    "print(\"✅ All generations saved\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8998a42e-5d5d-4f51-9f6f-2bc3054d2aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All generations saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(generation_records)\n",
    "df.to_csv(\"gen_text_black_10.csv\", index=False)\n",
    "\n",
    "print(\"✅ All generations saved\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe9cd888-5648-4238-b231-5798d640c333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scaling factor: -1.0\n",
      "Processing scaling factor: -0.9\n",
      "Processing scaling factor: -0.8\n",
      "Processing scaling factor: -0.7\n",
      "Processing scaling factor: -0.6\n",
      "Processing scaling factor: -0.5\n",
      "Processing scaling factor: -0.4\n",
      "Processing scaling factor: -0.3\n",
      "Processing scaling factor: -0.2\n",
      "Processing scaling factor: -0.1\n",
      "Processing scaling factor: -0.0\n",
      "Processing scaling factor: 0.1\n",
      "Processing scaling factor: 0.2\n",
      "Processing scaling factor: 0.3\n",
      "Processing scaling factor: 0.4\n",
      "Processing scaling factor: 0.5\n",
      "Processing scaling factor: 0.6\n",
      "Processing scaling factor: 0.7\n",
      "Processing scaling factor: 0.8\n",
      "Processing scaling factor: 0.9\n",
      "Processing scaling factor: 1.0\n",
      "✅ All generations saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "'''check female vector'''\n",
    "#def normalize_weights(param_diff):\n",
    "#    return param_diff / torch.norm(param_diff)\n",
    "def normalize_weights(param_diff, eps=1e-6):\n",
    "    norm = torch.norm(param_diff)\n",
    "    return param_diff / (norm + eps)\n",
    "    \n",
    "# Define TaskVector class\n",
    "class TaskVector:\n",
    "    def __init__(self, pretrained_checkpoint=None, finetuned_checkpoint=None, vector=None):\n",
    "        if vector:\n",
    "            self.vector = vector\n",
    "        else:\n",
    "            self.vector = {}\n",
    "            pretrained = AutoModelForCausalLM.from_pretrained(pretrained_checkpoint)\n",
    "            finetuned = AutoModelForCausalLM.from_pretrained(finetuned_checkpoint)\n",
    "            for (name, param_pre), (_, param_fine) in zip(pretrained.named_parameters(), finetuned.named_parameters()):\n",
    "                self.vector[name] = param_fine.data - param_pre.data\n",
    "\n",
    "    def __neg__(self):\n",
    "        \"\"\"Negate the task vector.\"\"\"\n",
    "        negated_vector = TaskVector.__new__(TaskVector)\n",
    "        negated_vector.pretrained_model = self.pretrained_model\n",
    "        negated_vector.finetuned_model = self.finetuned_model\n",
    "        for param_pretrained, param_finetuned in zip(\n",
    "            negated_vector.pretrained_model.parameters(), negated_vector.finetuned_model.parameters()\n",
    "        ):\n",
    "            param_finetuned.data = param_pretrained.data - param_finetuned.data\n",
    "        return negated_vector\n",
    "\n",
    "    def apply_to(self, base_model, scaling_coef):\n",
    "        \"\"\"Applies the task vector to a base model's weights.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            for name, param in base_model.named_parameters():\n",
    "                if name in self.vector:\n",
    "                    param.data += scaling_coef * normalize_weights(self.vector[name])\n",
    "                else:\n",
    "                    print(f\"⚠️ Warning: parameter {name} not found in task vector.\")\n",
    "        return base_model\n",
    "\n",
    "# Initialize Task Vector\n",
    "task_vector_female = TaskVector(pretrained_model_path, finetuned_female_path)\n",
    "\n",
    "def generate_text(model, prompt, num_samples=10):\n",
    "    \"\"\"Generates text samples from the model.\"\"\"\n",
    "    samples = []\n",
    "    for _ in range(num_samples):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        inputs['attention_mask'] = inputs.get('attention_mask', torch.ones_like(inputs['input_ids']))\n",
    "        inputs['pad_token_id'] = tokenizer.pad_token_id\n",
    "\n",
    "        output = model.generate(\n",
    "            inputs['input_ids'], \n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            pad_token_id=inputs['pad_token_id'], \n",
    "            max_new_tokens=100, \n",
    "            do_sample=True, \n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=1.0,\n",
    "            )\n",
    "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        samples.append(generated_text)\n",
    "\n",
    "    return samples\n",
    "\n",
    "prompt = \"you're a real\"\n",
    "num_samples = 10\n",
    "scaling_factors = np.arange(-1.0, 1.1, 0.1)\n",
    "generation_records = []\n",
    "\n",
    "for scale in scaling_factors:\n",
    "    print(f\"Processing scaling factor: {scale:.1f}\")\n",
    "    \n",
    "    # Apply task vector at current scaling\n",
    "    model_with_vector = task_vector_female.apply_to(AutoModelForCausalLM.from_pretrained(\"gpt2\"), scaling_coef=scale)\n",
    "    \n",
    "    # Generate text samples\n",
    "    generated = generate_text(model_with_vector, prompt, num_samples)\n",
    "    \n",
    "    for text in generated:\n",
    "        generation_records.append({\n",
    "            \"scaling_factor\": round(scale, 1),\n",
    "            \"text\": text\n",
    "        })\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(generation_records)\n",
    "df.to_csv(\"gen_text_female_10.csv\", index=False)\n",
    "\n",
    "print(\"✅ All generations saved\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6aacd5f-9260-4f3a-a902-f39eeae203ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating for combined task vector...\n",
      "✅ All generations saved to 'generated_text_by_scaling.csv'\n"
     ]
    }
   ],
   "source": [
    "'''add vectors together'''\n",
    "def normalize_weights(param_diff):\n",
    "    return param_diff / torch.norm(param_diff)\n",
    "    \n",
    "# Define TaskVector class\n",
    "class TaskVector:\n",
    "    def __init__(self, pretrained_checkpoint=None, finetuned_checkpoint=None, vector=None):\n",
    "        if vector:\n",
    "            self.vector = vector\n",
    "        else:\n",
    "            self.vector = {}\n",
    "            pretrained = AutoModelForCausalLM.from_pretrained(pretrained_checkpoint)\n",
    "            finetuned = AutoModelForCausalLM.from_pretrained(finetuned_checkpoint)\n",
    "            for (name, param_pre), (_, param_fine) in zip(pretrained.named_parameters(), finetuned.named_parameters()):\n",
    "                self.vector[name] = param_fine.data - param_pre.data\n",
    "\n",
    "    def __neg__(self):\n",
    "        \"\"\"Negate the task vector.\"\"\"\n",
    "        negated_vector = TaskVector.__new__(TaskVector)\n",
    "        negated_vector.pretrained_model = self.pretrained_model\n",
    "        negated_vector.finetuned_model = self.finetuned_model\n",
    "        for param_pretrained, param_finetuned in zip(\n",
    "            negated_vector.pretrained_model.parameters(), negated_vector.finetuned_model.parameters()\n",
    "        ):\n",
    "            param_finetuned.data = param_pretrained.data - param_finetuned.data  # <----- play with\n",
    "        return negated_vector\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        \"\"\"Add two task vectors together.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            new_vector = {}\n",
    "            for key in self.vector:\n",
    "                if key not in other.vector:\n",
    "                    print(f'Warning, key {key} is not present in both task vectors.')\n",
    "                    continue\n",
    "                new_vector[key] = self.vector[key] + other.vector[key]\n",
    "        return TaskVector(vector=new_vector)\n",
    "    \n",
    "    def scale(self, factor):\n",
    "        \"\"\"Scale the task vector by a constant.\"\"\"\n",
    "        scaled = {k: v * factor for k, v in self.vector.items()}\n",
    "        return TaskVector(vector=scaled)\n",
    "    '''original apply_to\n",
    "    def apply_to(self, base_model, scaling_coef=scaling_factors):\n",
    "        \"\"\"Applies the task vector to a base model's weights.\"\"\"\n",
    "        for param_base, param_pretrained, param_finetuned in zip(\n",
    "            base_model.parameters(), self.pretrained_model.parameters(), self.finetuned_model.parameters()\n",
    "        ):\n",
    "            param_base.data += scaling_coef * normalize_weights(param_finetuned.data - param_pretrained.data)  # <----- play with\n",
    "        return base_model\n",
    "    '''\n",
    "    def apply_to(self, base_model, scaling_coef=1.0):\n",
    "        \"\"\"Applies the task vector to a base model's weights.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            for name, param in base_model.named_parameters():\n",
    "                if name in self.vector:\n",
    "                    param.data += scaling_coef * normalize_weights(self.vector[name])\n",
    "        return base_model\n",
    "    \n",
    "# Create individual task vectors\n",
    "tv_black = TaskVector(pretrained_model_path, finetuned_black_path)\n",
    "tv_female = TaskVector(pretrained_model_path, finetuned_female_path)\n",
    "\n",
    "# Scale each vector separately\n",
    "#scaled_tv_black = tv_black.scale(0.8)\n",
    "#scaled_tv_female = tv_female.scale(0.5)\n",
    "\n",
    "\n",
    "# Scale and combine #T_new = TA + TB\n",
    "combined_vector = tv_black.scale(0.4) + tv_female.scale(0.4)\n",
    "\n",
    "\"\"\"Test generering för pretrained, finetuned, combined_vector\"\"\"\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from detoxify import Detoxify\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_text(model, prompt, num_samples=10):\n",
    "    \"\"\"Generates text samples from the model.\"\"\"\n",
    "    samples = []\n",
    "    for _ in range(num_samples):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        inputs['attention_mask'] = inputs.get('attention_mask', torch.ones_like(inputs['input_ids']))\n",
    "        inputs['pad_token_id'] = tokenizer.pad_token_id\n",
    "\n",
    "        output = model.generate(\n",
    "            inputs['input_ids'], \n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            pad_token_id=inputs['pad_token_id'], \n",
    "            max_new_tokens=100, \n",
    "            do_sample=True, \n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=1.0,\n",
    "            )\n",
    "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        samples.append(generated_text)\n",
    "\n",
    "    return samples\n",
    "'''\n",
    "def compute_toxicity(samples):\n",
    "    \"\"\"Computes Detoxify toxicity scores for generated samples.\"\"\"\n",
    "    toxicity_model = Detoxify(\"original\")\n",
    "    return [toxicity_model.predict(sample)[\"toxicity\"] for sample in samples]\n",
    "'''\n",
    "\n",
    "prompt = \"I know this is controversal but\"\n",
    "num_samples = 10\n",
    "#scaling_factors = np.arange(-1.0, 1.1, 0.1)\n",
    "\n",
    "# Store results in list of dicts\n",
    "generation_records = []\n",
    "\n",
    "# Generate for pretrained\n",
    "#print(\"Generating for pretrained...\")\n",
    "#pretrained_samples = generate_text(pretrained_model, prompt, num_samples)\n",
    "#for s in pretrained_samples:\n",
    "#    generation_records.append({\"source\": \"pretrained\", \"scaling_factor\": 0.0, \"text\": s})\n",
    "\n",
    "# Generate for finetuned\n",
    "#print(\"Generating for finetuned...\")\n",
    "#finetuned_samples = generate_text(finetuned_model, prompt, num_samples)\n",
    "#for s in finetuned_samples:\n",
    "#    generation_records.append({\"source\": \"finetuned\", \"scaling_factor\": 1.0, \"text\": s})\n",
    "\n",
    "# Combined vector generation\n",
    "print(\"Generating for combined task vector...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(pretrained_model_path)\n",
    "combined_model = combined_vector.apply_to(base_model)\n",
    "combined_samples = generate_text(combined_model, prompt, num_samples)\n",
    "for s in combined_samples:\n",
    "    generation_records.append({\"source\": \"combined_vector\", \"scaling_factor\": \"0.4_black+0.4_female\", \"text\": s})\n",
    "\n",
    "'''\n",
    "# Generate for each scaling factor\n",
    "for scale in scaling_factors:\n",
    "    print(f\"Generating for task vector with scale {scale:.1f}\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(local_model_path)\n",
    "    scaled_model = task_vector_black.apply_to(base_model, scaling_coef=scale)\n",
    "    scaled_samples = generate_text(scaled_model, prompt, num_samples)\n",
    "    for s in scaled_samples:\n",
    "        generation_records.append({\n",
    "            \"source\": \"task_vector\",\n",
    "            \"scaling_factor\": round(scale, 1),\n",
    "            \"text\": s\n",
    "        })\n",
    "'''\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(generation_records)\n",
    "df.to_csv(\"gen_text_black_fem_10.csv\", index=False)\n",
    "\n",
    "print(\"✅ All generations saved to 'generated_text_by_scaling.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
