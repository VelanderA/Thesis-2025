{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fc0058d-006d-4bae-9506-a7cdea571ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266852fc3b114cff94c563c471c0f12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|███████████████████████████████████████████████████████████| 21.0M/21.0M [00:01<00:00, 17.6MB/s]\n",
      "Downloading data: 100%|███████████████████████████████████████████████████████████| 20.5M/20.5M [00:00<00:00, 26.0MB/s]\n",
      "Downloading data: 100%|███████████████████████████████████████████████████████████| 42.0M/42.0M [00:01<00:00, 34.9MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c7dd24cfab415daabc64f73cca3892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3dda675e1994e10bb2b71d5bf75bd9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8313e1cce06549ad91b63a10551f54a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Download Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the IMDb dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Use only the training split for fine-tuning\n",
    "train_texts = dataset[\"train\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d16338be-7840-417b-a4c0-8cd273506f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e4bd0421b8435ca26124795bf78819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Tokenize the Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load GPT-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Assign padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Use the end-of-sequence token as the padding token\n",
    "\n",
    "# Tokenize the text\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=128, padding=\"max_length\")\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "tokenized_dataset = dataset[\"train\"].map(tokenize_function, batched=True)\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5447e38f-b87c-4225-b20b-e17d67a6d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare DataLoaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create DataLoader for the training set\n",
    "train_dataloader = DataLoader(tokenized_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71a0d657-bd91-4f8f-acb8-b006ca755c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3949cf883c6b49459abbf72f60483998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb378771bec42faac03706cac265722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anneth\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Load GPT-2 and Define Training Loop\n",
    "from transformers import GPT2LMHeadModel, AdamW\n",
    "import torch\n",
    "\n",
    "# Load the GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04eecc88-60d4-41bb-8f42-68d8ebf831f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./pretrained_gpt2\\\\tokenizer_config.json',\n",
       " './pretrained_gpt2\\\\special_tokens_map.json',\n",
       " './pretrained_gpt2\\\\vocab.json',\n",
       " './pretrained_gpt2\\\\merges.txt',\n",
       " './pretrained_gpt2\\\\added_tokens.json',\n",
       " './pretrained_gpt2\\\\tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./pretrained_gpt2\")\n",
    "tokenizer.save_pretrained(\"./pretrained_gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0eb2820-d48e-490d-86a6-8951826819b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████████████████████████████████████████████████████| 3125/3125 [1:31:11<00:00,  1.75s/it, loss=3.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████████████████████| 3125/3125 [2:09:39<00:00,  2.49s/it, loss=3.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|████████████████████████████████████████████████████████| 3125/3125 [2:03:28<00:00,  2.37s/it, loss=3.28]\n"
     ]
    }
   ],
   "source": [
    "#Fine-Tune GPT-2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Fine-tuning loop\n",
    "model.train()\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    loop = tqdm(train_dataloader, leave=True)\n",
    "    for batch in loop:\n",
    "        # Move batch to GPU\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Print loss\n",
    "        loop.set_description(f\"Epoch {epoch}\")\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "043b1e92-b145-4dc4-8da1-274535c50bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./finetuned_gpt2\\\\tokenizer_config.json',\n",
       " './finetuned_gpt2\\\\special_tokens_map.json',\n",
       " './finetuned_gpt2\\\\vocab.json',\n",
       " './finetuned_gpt2\\\\merges.txt',\n",
       " './finetuned_gpt2\\\\added_tokens.json',\n",
       " './finetuned_gpt2\\\\tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save the Fine-Tuned Model\n",
    "model.save_pretrained(\"./finetuned_gpt2\")\n",
    "tokenizer.save_pretrained(\"./finetuned_gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77f80451-c987-405a-b535-2d92aa42b75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: This is a simple sentence and not that long.\n",
      "Output (Fine-Tuned Model): This is a simple sentence and not that long. It's about the life of an American who has been in prison for over 20 years, but he doesn't know what to do with it.<br /><b />The film is based on real events from his childhood - which are very disturbing. The story is about how this man was raised by\n",
      "Output (After Task Vector Adjustment): This is a simple sentence and not that long. It does have some grammar but it has enough grammatical structure to make sentences easy for comprehension of the whole thing without any further Grammar Structure\n",
      "I am going on here because I want you understand how this works so well then if i dont know what im gonna do now just go ahead let me explain\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer\n",
    "import nbimporter\n",
    "from task_vectors import TaskVector\n",
    "from safetensors.torch import load_file\n",
    "from transformers import pipeline\n",
    "from task_vectors import TaskVector\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Fine-tuned model (assume you have a fine-tuned model)\n",
    "finetuned_model_path = \"./finetuned_gpt2\"  # Replace with your model path\n",
    "finetuned_model = GPT2LMHeadModel.from_pretrained(finetuned_model_path)\n",
    "\n",
    "def normalize_weights(param_diff):\n",
    "    return param_diff / torch.norm(param_diff)\n",
    "    \n",
    "# Define TaskVector class\n",
    "class TaskVector:\n",
    "    def __init__(self, pretrained_checkpoint, finetuned_checkpoint):\n",
    "        self.pretrained_model = GPT2LMHeadModel.from_pretrained(pretrained_checkpoint)\n",
    "        self.finetuned_model = GPT2LMHeadModel.from_pretrained(finetuned_checkpoint)\n",
    "\n",
    "    def __neg__(self):\n",
    "        \"\"\"Negate the task vector.\"\"\"\n",
    "        negated_vector = TaskVector.__new__(TaskVector)\n",
    "        negated_vector.pretrained_model = self.pretrained_model\n",
    "        negated_vector.finetuned_model = self.finetuned_model\n",
    "        for param_pretrained, param_finetuned in zip(\n",
    "            negated_vector.pretrained_model.parameters(), negated_vector.finetuned_model.parameters()\n",
    "        ):\n",
    "            param_finetuned.data = param_pretrained.data - param_finetuned.data\n",
    "        return negated_vector\n",
    "\n",
    "    def apply_to(self, base_model):\n",
    "        \"\"\"Applies the task vector to a base model's weights.\"\"\"\n",
    "        for param_base, param_pretrained, param_finetuned in zip(\n",
    "            base_model.parameters(), self.pretrained_model.parameters(), self.finetuned_model.parameters()\n",
    "        ):\n",
    "            param_base.data += scaling_coef * normalize_weights(param_finetuned.data - param_pretrained.data)\n",
    "        return base_model\n",
    "\n",
    "# Initialize TaskVector\n",
    "task_vector = TaskVector(\"gpt2\", finetuned_model_path)\n",
    "\n",
    "# Negate the Task Vector to adjust toward negative sentiment\n",
    "neg_task_vector = -task_vector\n",
    "\n",
    "# Sentence to transfer\n",
    "input_sentence = \"This is a simple sentence and not that long.\"\n",
    "\n",
    "# Generate output using the fine-tuned model\n",
    "def generate_with_model(model, sentence):\n",
    "    inputs = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "    output = model.generate(\n",
    "    inputs,\n",
    "    max_length=70,          # Limit output length\n",
    "    temperature=0.7,         # Adjust temperature for randomness\n",
    "    top_k=85,                # Limit sampling to top k candidates\n",
    "    top_p=0.9,               # Nucleus sampling for diversity\n",
    "    repetition_penalty=3.0,  # Apply repetition penalty\n",
    "    num_return_sequences=1   # Only generate one sequence\n",
    "    )\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Generate output after applying task vector\n",
    "base_model_copy = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "scaling_coef = 0.8  # Adjust the scaling factor as needed\n",
    "task_adjusted_model = neg_task_vector.apply_to(base_model_copy)\n",
    "\n",
    "# Generate sentences\n",
    "finetuned_output = generate_with_model(finetuned_model, input_sentence)\n",
    "adjusted_output = generate_with_model(task_adjusted_model, input_sentence)\n",
    "\n",
    "# Print input and outputs\n",
    "print(\"Input Sentence:\", input_sentence)\n",
    "print(\"Output (Fine-Tuned Model):\", finetuned_output)\n",
    "print(\"Output (After Task Vector Adjustment):\", adjusted_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
