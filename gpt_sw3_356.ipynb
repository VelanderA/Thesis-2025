{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a45616-756e-4b38-a7cc-026b39a2ba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load sw3-356-instruct\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Model name\n",
    "model_name = \"AI-Sweden-Models/gpt-sw3-356m-instruct\"\n",
    "\n",
    "# Determine the device to use (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9848116c-418b-42c1-8af9-091c994b73f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anneth\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 2048}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"./pretrained_sw3_356\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36820595-5a46-4d2b-89da-792b58e86a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(\"synthetic_data_fixed.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a47c7b92-2934-4fcb-98a6-09bc8af3f0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion completed: converted_data.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Read your data file\n",
    "with open(\"synthetic_data_fixed.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "cleaned_data = []\n",
    "\n",
    "# Regex to remove unwanted text parts\n",
    "pattern = r'text\"?\\[\"?|text\"?\\]?'\n",
    "\n",
    "# Convert to JSON structure\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if line:\n",
    "        # Remove unwanted markers and symbols\n",
    "        clean_line = re.sub(pattern, '', line)\n",
    "        # Only add non-empty lines\n",
    "        if clean_line:\n",
    "            cleaned_data.append({\"text\": clean_line})\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"converted_data.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(cleaned_data, json_file, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "print(\"Conversion completed: converted_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f8b2993-59e7-4b65-8bec-d63570cd2fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully: ['text']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695cb0512aa3401d826f5e2fe2f73b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/527 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '[', 'input_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 398], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'labels': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 398]}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Load synthetic_data_fixed.json and tokenize\"\"\"\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the cleaned JSON file\n",
    "data_path = \"converted_data.json\"\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)  # Load as a list of dictionaries\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "train_df = pd.DataFrame(data)\n",
    "\n",
    "# Ensure 'text' column exists\n",
    "if \"text\" not in train_df.columns:\n",
    "    raise ValueError(\"The JSON file does not contain a 'text' column.\")\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(train_df)\n",
    "print(\"Dataset loaded successfully:\", dataset.column_names)\n",
    "\n",
    "# Load the tokenizer\n",
    "local_model_path = r\"C:\\models--AI-Sweden-Models--gpt-sw3-356m-instruct\\snapshots\\fce932486e4fa09d377ff8a499f0a2b6145efbf7\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    tokens = tokenizer(\n",
    "        examples['text'], \n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()  # Add labels\n",
    "    return tokens\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Print example tokenized output\n",
    "print(tokenized_dataset[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee35e66b-df17-43af-bc3f-6c79501932e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': '['}, {'text': '\"â€“ Josefina Sundin sitter ofta i utsatta miljoÌˆer, beraÌˆttar Greta JÃ¶nsson. SaÌŠ om det fungerar foÌˆr oss vintertid saÌŠ finns det inga hinder foÌˆr att utoÌˆka systemet till andra kommunala verksamheter. Gunnar Lindberg har baÌŠde bokstavligt och bildlikt en nyckelroll paÌŠ Dalavatten:\",'}, {'text': '\"â€“ All kommunikation kring projektet sker direkt i Openings Studio, sÃ¤ger Britt-Marie Persson. Det finns inte lÃ¤ngre nÃ¥got behov av att skicka mejl om till exempel fÃ¶rÃ¤ndringar i projektet. Allt hanteras och spÃ¥ras i mjukvaran.\",'}, {'text': '\"Ulf Hansson in i Karlsson HB Karlsson HBs vÃ¤rld av lÃ¥ssystem och lÃ¤r dig om fÃ¶rdelar, funktioner och finesser.\",'}, {'text': '\"â€“ Johanna Henriksson hade vi ett nyckelskaÌŠp fraÌŠn en annan leverantoÌˆr. Med- arbetarna knappade aÌˆven daÌŠ in en kod naÌˆr de skulle haÌˆmta nycklar men maÌŠste kvittera haÌˆmtning och aÌŠterlaÌˆmning paÌŠ\",'}, {'text': '\"Remote, dÃ¤r det rÃ¤cker med en nyckel, berÃ¤ttar Viktoria Olausson, drift- och sÃ¤kerhetsansvarig pÃ¥ Statens maritima museer.\",'}, {'text': '\"Beslagning fÃ¶r dÃ¶rrmiljÃ¶er Ã¤r ett omrÃ¥de som blir allt mer komplext. Hanteringen av dokumentationen i ett projekt som innefattar dÃ¶rrmiljÃ¶er kan bli omfattande â€“ fÃ¶r att inte tala om den kommunikation som Ã¤r nÃ¶dvÃ¤ndig mellan intressenterna i projektet. Mjukvaran Openings Studio frÃ¥n Karlsson HB Karlsson HB har gjort stor skillnad fÃ¶r arkitekten Ulrika Johansson.\",'}, {'text': '\"â€“ Vi Ã¤r ett vÃ¤rldsledande bolag i vÃ¥r bransch, sÃ¥ det Ã¤r klart att vi vÃ¤ljer sÃ¤kerhetslÃ¶sningar frÃ¥n Karlsson HB Karlsson HB, sÃ¤ger Magnus Persson, fastighetsansvarig pÃ¥ CTEK.\",'}, {'text': '\"â€“ Inom hemtjaÌˆnsten maÌŠste alla ha tillgaÌŠng till samtliga nycklar och vi daÌˆrfoÌˆr kan vi inte ha laÌŠsta positioner, menar Stig Svensson. Rekommenderar du Traka Touch till andra utfoÌˆrare inom hemtjaÌˆnsten? â€“ Det goÌˆr jag definitivt och Olivia HemtjaÌˆnst har redan bestaÌˆllt flera skaÌŠp till fyra av vaÌŠra andra kontor i Stockholm.\",'}, {'text': '\"Karlsson HB Karlsson HB Opening Solutions Sverige YouTube.\",'}]\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da48e450-9eac-4464-9031-0dedbf21bfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"load data\"\"\"\n",
    "local_model_path = r\"C:\\models--AI-Sweden-Models--gpt-sw3-356m-instruct\\snapshots\\fce932486e4fa09d377ff8a499f0a2b6145efbf7\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load tokenizer and model from the local directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(local_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "650349e0-32ee-4fcc-952e-aea9c51239a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in c:\\users\\anneth\\appdata\\roaming\\python\\python311\\site-packages (0.14.0)\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\anneth\\anaconda3\\envs\\torch\\lib\\site-packages (0.45.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\anneth\\anaconda3\\envs\\torch\\lib\\site-packages (from peft) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\anneth\\anaconda3\\envs\\torch\\lib\\site-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\anneth\\appdata\\roaming\\python\\python311\\site-packages (from peft) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\anneth\\appdata\\roaming\\python\\python311\\site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\anneth\\appdata\\roaming\\python\\python311\\site-packages (from peft) (2.5.1+cu121)\n",
      "Requirement already satisfied: transformers in c:\\users\\anneth\\appdata\\roaming\\python\\python311\\site-packages (from peft) (4.47.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\anneth\\anaconda3\\envs\\torch\\lib\\site-packages (from peft) (4.65.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\anneth\\appdata\\roaming\\python\\python311\\site-packages (from peft) (1.2.1)\n",
      "Requirement already satisfied: safetensors in c:\\users\\anneth\\appdata\\roaming\\python\\python311\\site-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.0 in c:\\users\\anneth\\appdata\\roaming\\python\\python311\\site-packages (from peft) (0.27.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\anneth\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.25.0->peft) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\anneth\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.25.0->peft) (2024.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\anneth\\anaconda3\\envs\\torch\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\anneth\\anaconda3\\envs\\torch\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\anneth\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.13.0->peft) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\anneth\\anaconda3\\envs\\torch\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\anneth\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\anneth\\appdata\\roaming\\python\\python311\\site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\anneth\\anaconda3\\envs\\torch\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\anneth\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers->peft) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\anneth\\appdata\\roaming\\python\\python311\\site-packages (from transformers->peft) (0.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\anneth\\anaconda3\\envs\\torch\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\anneth\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anneth\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\anneth\\appdata\\roaming\\python\\python311\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anneth\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (2024.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\Anneth\\anaconda3\\envs\\torch\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\Anneth\\anaconda3\\envs\\torch\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\Anneth\\anaconda3\\envs\\torch\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\"\"\"for memory efficiency\"\"\"\n",
    "!pip install peft bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f352f6b6-4504-481c-ac2e-0f771eadf22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anneth\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Set Up Training Arguments\"\"\"\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt-sw3-finetuned\",  # Directory for model checkpoints\n",
    "    evaluation_strategy=\"no\",       # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",             # Save model checkpoints every epoch\n",
    "    per_device_train_batch_size=1,     # Adjust based on your GPU memory\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=3,                # Number of epochs (adjustable)\n",
    "    logging_dir=\"./logs\",              # Log directory\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,                 # Keeps last 2 checkpoints only\n",
    "    fp16=True,                          # Enable if using GPU with FP16 support\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "feb63db3-ad5e-4b00-8df7-8e462b367c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anneth\\AppData\\Local\\Temp\\ipykernel_19752\\2380114429.py:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Initialize Trainer\"\"\"\n",
    "from transformers import Trainer, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(local_model_path)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,  # Now with labels\n",
    "    tokenizer=tokenizer,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "490694e1-f068-42db-b07e-9982a980b00d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1581' max='1581' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1581/1581 4:57:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.592500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.498600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.353200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.345400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.320400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.219100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.173300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.162300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.142900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.154500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.105000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.080300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.078800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.061200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.055100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anneth\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 2048}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1581, training_loss=0.34095062114708335, metrics={'train_runtime': 17840.0583, 'train_samples_per_second': 0.089, 'train_steps_per_second': 0.089, 'total_flos': 1468275798048768.0, 'train_loss': 0.34095062114708335, 'epoch': 3.0})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Start Fine-Tuning\"\"\"\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b85a41fd-21ed-4543-abd1-bab4dfdd492d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./finetuned_sw3_356\\\\tokenizer_config.json',\n",
       " './finetuned_sw3_356\\\\special_tokens_map.json',\n",
       " './finetuned_sw3_356\\\\spiece.model',\n",
       " './finetuned_sw3_356\\\\added_tokens.json')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save the Fine-Tuned Model\n",
    "model.save_pretrained(\"./finetuned_sw3_356\")\n",
    "tokenizer.save_pretrained(\"./finetuned_sw3_356\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94480d80-27f3-407d-a8f9-748bc36ec54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TASK VECTOR OPERATION\"\"\"\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer\n",
    "import nbimporter\n",
    "from task_vectors import TaskVector\n",
    "from safetensors.torch import load_file\n",
    "from transformers import pipeline\n",
    "from task_vectors import TaskVector\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\") # <----- change\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")  # <----- change\n",
    "\n",
    "# Fine-tuned model (assume you have a fine-tuned model)\n",
    "finetuned_model_path = \"./finetuned_gpt2\"   # <----- change\n",
    "finetuned_model = GPT2LMHeadModel.from_pretrained(finetuned_model_path)\n",
    "\n",
    "def normalize_weights(param_diff):\n",
    "    return param_diff / torch.norm(param_diff)\n",
    "    \n",
    "# Define TaskVector class\n",
    "class TaskVector:\n",
    "    def __init__(self, pretrained_checkpoint, finetuned_checkpoint):\n",
    "        self.pretrained_model = GPT2LMHeadModel.from_pretrained(pretrained_checkpoint)\n",
    "        self.finetuned_model = GPT2LMHeadModel.from_pretrained(finetuned_checkpoint)\n",
    "\n",
    "    def __neg__(self):\n",
    "        \"\"\"Negate the task vector.\"\"\"\n",
    "        negated_vector = TaskVector.__new__(TaskVector)\n",
    "        negated_vector.pretrained_model = self.pretrained_model\n",
    "        negated_vector.finetuned_model = self.finetuned_model\n",
    "        for param_pretrained, param_finetuned in zip(\n",
    "            negated_vector.pretrained_model.parameters(), negated_vector.finetuned_model.parameters()\n",
    "        ):\n",
    "            param_finetuned.data = param_pretrained.data - param_finetuned.data  # <----- play with\n",
    "        return negated_vector\n",
    "\n",
    "    def apply_to(self, base_model):\n",
    "        \"\"\"Applies the task vector to a base model's weights.\"\"\"\n",
    "        for param_base, param_pretrained, param_finetuned in zip(\n",
    "            base_model.parameters(), self.pretrained_model.parameters(), self.finetuned_model.parameters()\n",
    "        ):\n",
    "            param_base.data += scaling_coef * normalize_weights(param_finetuned.data - param_pretrained.data)  # <----- play with\n",
    "        return base_model\n",
    "\n",
    "# Initialize TaskVector\n",
    "task_vector = TaskVector(\"gpt2\", finetuned_model_path)  # <----- change\n",
    "\n",
    "# Negate the Task Vector to adjust toward negative sentiment\n",
    "neg_task_vector = -task_vector  # <----- play with\n",
    "\n",
    "# Sentence to transfer\n",
    "input_sentence = \"This is a simple sentence and not that long.\"  # <----- change\n",
    "\"\"\"\n",
    "prompt should look like this:\n",
    "[Instruction]: Provide a clear and concise task or question.\n",
    "[Context]: Include any necessary background information or examples.\n",
    "[Format]: Specify the desired response format (bullet points, paragraphs, JSON, etc.).\n",
    "[Constraints]: (Optional) Set any rules, word limits, or tone preferences.\n",
    "\"\"\"\n",
    "\n",
    "# Generate output using the fine-tuned model\n",
    "def generate_with_model(model, sentence):  # <----- play with\n",
    "    inputs = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "    output = model.generate(\n",
    "    inputs,\n",
    "    max_length=70,          # Limit output length\n",
    "    temperature=0.7,         # Adjust temperature for randomness\n",
    "    top_k=85,                # Limit sampling to top k candidates\n",
    "    top_p=0.9,               # Nucleus sampling for diversity\n",
    "    repetition_penalty=3.0,  # Apply repetition penalty\n",
    "    num_return_sequences=1   # Only generate one sequence\n",
    "    )\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Generate output after applying task vector\n",
    "base_model_copy = GPT2LMHeadModel.from_pretrained(\"gpt2\")  # <----- change\n",
    "scaling_coef = 0.8  # Adjust the scaling factor as needed\n",
    "task_adjusted_model = neg_task_vector.apply_to(base_model_copy)\n",
    "\n",
    "# Generate sentences\n",
    "finetuned_output = generate_with_model(finetuned_model, input_sentence)\n",
    "adjusted_output = generate_with_model(task_adjusted_model, input_sentence)\n",
    "\n",
    "# Print input and outputs\n",
    "print(\"Input Sentence:\", input_sentence)\n",
    "print(\"Output (Fine-Tuned Model):\", finetuned_output)\n",
    "print(\"Output (After Task Vector Adjustment):\", adjusted_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
