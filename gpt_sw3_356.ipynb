{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a45616-756e-4b38-a7cc-026b39a2ba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load sw3-356-instruct\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Model name\n",
    "model_name = \"AI-Sweden-Models/gpt-sw3-356m-instruct\"\n",
    "\n",
    "# Determine the device to use (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9848116c-418b-42c1-8af9-091c994b73f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anneth\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 2048}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"./pretrained_sw3_356\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36820595-5a46-4d2b-89da-792b58e86a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(\"synthetic_data_fixed.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a47c7b92-2934-4fcb-98a6-09bc8af3f0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion completed: converted_data.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Read your data file\n",
    "with open(\"synthetic_data_fixed.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "cleaned_data = []\n",
    "\n",
    "# Regex to remove unwanted text parts\n",
    "pattern = r'text\"?\\[\"?|text\"?\\]?'\n",
    "\n",
    "# Convert to JSON structure\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if line:\n",
    "        # Remove unwanted markers and symbols\n",
    "        clean_line = re.sub(pattern, '', line)\n",
    "        # Only add non-empty lines\n",
    "        if clean_line:\n",
    "            cleaned_data.append({\"text\": clean_line})\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"converted_data.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(cleaned_data, json_file, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "print(\"Conversion completed: converted_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f8b2993-59e7-4b65-8bec-d63570cd2fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully: ['text']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695cb0512aa3401d826f5e2fe2f73b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/527 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '[', 'input_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 398], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'labels': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 398]}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Load synthetic_data_fixed.json and tokenize\"\"\"\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the cleaned JSON file\n",
    "data_path = \"converted_data.json\"\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)  # Load as a list of dictionaries\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "train_df = pd.DataFrame(data)\n",
    "\n",
    "# Ensure 'text' column exists\n",
    "if \"text\" not in train_df.columns:\n",
    "    raise ValueError(\"The JSON file does not contain a 'text' column.\")\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(train_df)\n",
    "print(\"Dataset loaded successfully:\", dataset.column_names)\n",
    "\n",
    "# Load the tokenizer\n",
    "local_model_path = r\"C:\\models--AI-Sweden-Models--gpt-sw3-356m-instruct\\snapshots\\fce932486e4fa09d377ff8a499f0a2b6145efbf7\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    tokens = tokenizer(\n",
    "        examples['text'], \n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()  # Add labels\n",
    "    return tokens\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Print example tokenized output\n",
    "print(tokenized_dataset[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee35e66b-df17-43af-bc3f-6c79501932e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': '['}, {'text': '\"– Josefina Sundin sitter ofta i utsatta miljöer, berättar Greta Jönsson. Så om det fungerar för oss vintertid så finns det inga hinder för att utöka systemet till andra kommunala verksamheter. Gunnar Lindberg har både bokstavligt och bildlikt en nyckelroll på Dalavatten:\",'}, {'text': '\"– All kommunikation kring projektet sker direkt i Openings Studio, säger Britt-Marie Persson. Det finns inte längre något behov av att skicka mejl om till exempel förändringar i projektet. Allt hanteras och spåras i mjukvaran.\",'}, {'text': '\"Ulf Hansson in i Karlsson HB Karlsson HBs värld av låssystem och lär dig om fördelar, funktioner och finesser.\",'}, {'text': '\"– Johanna Henriksson hade vi ett nyckelskåp från en annan leverantör. Med- arbetarna knappade även då in en kod när de skulle hämta nycklar men måste kvittera hämtning och återlämning på\",'}, {'text': '\"Remote, där det räcker med en nyckel, berättar Viktoria Olausson, drift- och säkerhetsansvarig på Statens maritima museer.\",'}, {'text': '\"Beslagning för dörrmiljöer är ett område som blir allt mer komplext. Hanteringen av dokumentationen i ett projekt som innefattar dörrmiljöer kan bli omfattande – för att inte tala om den kommunikation som är nödvändig mellan intressenterna i projektet. Mjukvaran Openings Studio från Karlsson HB Karlsson HB har gjort stor skillnad för arkitekten Ulrika Johansson.\",'}, {'text': '\"– Vi är ett världsledande bolag i vår bransch, så det är klart att vi väljer säkerhetslösningar från Karlsson HB Karlsson HB, säger Magnus Persson, fastighetsansvarig på CTEK.\",'}, {'text': '\"– Inom hemtjänsten måste alla ha tillgång till samtliga nycklar och vi därför kan vi inte ha låsta positioner, menar Stig Svensson. Rekommenderar du Traka Touch till andra utförare inom hemtjänsten? – Det gör jag definitivt och Olivia Hemtjänst har redan beställt flera skåp till fyra av våra andra kontor i Stockholm.\",'}, {'text': '\"Karlsson HB Karlsson HB Opening Solutions Sverige YouTube.\",'}]\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da48e450-9eac-4464-9031-0dedbf21bfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"load data\"\"\"\n",
    "local_model_path = r\"C:\\models--AI-Sweden-Models--gpt-sw3-356m-instruct\\snapshots\\fce932486e4fa09d377ff8a499f0a2b6145efbf7\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load tokenizer and model from the local directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(local_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "650349e0-32ee-4fcc-952e-aea9c51239a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in c:\\users\\anneth\\appdata\\roaming\\python\\python311\\site-packages (0.14.0)\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\anneth\\anaconda3\\envs\\torch\\lib\\site-packages (0.45.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\anneth\\anaconda3\\envs\\torch\\lib\\site-packages (from peft) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\anneth\\anaconda3\\envs\\torch\\lib\\site-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\anneth\\appdata\\roaming\\python\\python311\\site-packages (from peft) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\anneth\\appdata\\roaming\\python\\python311\\site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\anneth\\appdata\\roaming\\python\\python311\\site-packages (from peft) (2.5.1+cu121)\n",
      "Requirement already satisfied: transformers in c:\\users\\anneth\\appdata\\roaming\\python\\python311\\site-packages (from peft) (4.47.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\anneth\\anaconda3\\envs\\torch\\lib\\site-packages (from peft) (4.65.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\anneth\\appdata\\roaming\\python\\python311\\site-packages (from peft) (1.2.1)\n",
      "Requirement already satisfied: safetensors in c:\\users\\anneth\\appdata\\roaming\\python\\python311\\site-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.0 in c:\\users\\anneth\\appdata\\roaming\\python\\python311\\site-packages (from peft) (0.27.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\anneth\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.25.0->peft) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\anneth\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.25.0->peft) (2024.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\anneth\\anaconda3\\envs\\torch\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\anneth\\anaconda3\\envs\\torch\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\anneth\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.13.0->peft) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\anneth\\anaconda3\\envs\\torch\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\anneth\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\anneth\\appdata\\roaming\\python\\python311\\site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\anneth\\anaconda3\\envs\\torch\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\anneth\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers->peft) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\anneth\\appdata\\roaming\\python\\python311\\site-packages (from transformers->peft) (0.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\anneth\\anaconda3\\envs\\torch\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\anneth\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anneth\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\anneth\\appdata\\roaming\\python\\python311\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anneth\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (2024.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\Anneth\\anaconda3\\envs\\torch\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\Anneth\\anaconda3\\envs\\torch\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\Anneth\\anaconda3\\envs\\torch\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\"\"\"for memory efficiency\"\"\"\n",
    "!pip install peft bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f352f6b6-4504-481c-ac2e-0f771eadf22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anneth\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Set Up Training Arguments\"\"\"\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt-sw3-finetuned\",  # Directory for model checkpoints\n",
    "    evaluation_strategy=\"no\",       # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",             # Save model checkpoints every epoch\n",
    "    per_device_train_batch_size=1,     # Adjust based on your GPU memory\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=3,                # Number of epochs (adjustable)\n",
    "    logging_dir=\"./logs\",              # Log directory\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,                 # Keeps last 2 checkpoints only\n",
    "    fp16=True,                          # Enable if using GPU with FP16 support\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "feb63db3-ad5e-4b00-8df7-8e462b367c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anneth\\AppData\\Local\\Temp\\ipykernel_19752\\2380114429.py:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Initialize Trainer\"\"\"\n",
    "from transformers import Trainer, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(local_model_path)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,  # Now with labels\n",
    "    tokenizer=tokenizer,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "490694e1-f068-42db-b07e-9982a980b00d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1581' max='1581' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1581/1581 4:57:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.592500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.498600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.353200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.345400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.320400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.219100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.173300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.162300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.142900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.154500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.105000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.080300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.078800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.061200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.055100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anneth\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 2048}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1581, training_loss=0.34095062114708335, metrics={'train_runtime': 17840.0583, 'train_samples_per_second': 0.089, 'train_steps_per_second': 0.089, 'total_flos': 1468275798048768.0, 'train_loss': 0.34095062114708335, 'epoch': 3.0})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Start Fine-Tuning\"\"\"\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b85a41fd-21ed-4543-abd1-bab4dfdd492d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./finetuned_sw3_356\\\\tokenizer_config.json',\n",
       " './finetuned_sw3_356\\\\special_tokens_map.json',\n",
       " './finetuned_sw3_356\\\\spiece.model',\n",
       " './finetuned_sw3_356\\\\added_tokens.json')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save the Fine-Tuned Model\n",
    "model.save_pretrained(\"./finetuned_sw3_356\")\n",
    "tokenizer.save_pretrained(\"./finetuned_sw3_356\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94480d80-27f3-407d-a8f9-748bc36ec54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TASK VECTOR OPERATION\"\"\"\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer\n",
    "import nbimporter\n",
    "from task_vectors import TaskVector\n",
    "from safetensors.torch import load_file\n",
    "from transformers import pipeline\n",
    "from task_vectors import TaskVector\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\") # <----- change\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")  # <----- change\n",
    "\n",
    "# Fine-tuned model (assume you have a fine-tuned model)\n",
    "finetuned_model_path = \"./finetuned_gpt2\"   # <----- change\n",
    "finetuned_model = GPT2LMHeadModel.from_pretrained(finetuned_model_path)\n",
    "\n",
    "def normalize_weights(param_diff):\n",
    "    return param_diff / torch.norm(param_diff)\n",
    "    \n",
    "# Define TaskVector class\n",
    "class TaskVector:\n",
    "    def __init__(self, pretrained_checkpoint, finetuned_checkpoint):\n",
    "        self.pretrained_model = GPT2LMHeadModel.from_pretrained(pretrained_checkpoint)\n",
    "        self.finetuned_model = GPT2LMHeadModel.from_pretrained(finetuned_checkpoint)\n",
    "\n",
    "    def __neg__(self):\n",
    "        \"\"\"Negate the task vector.\"\"\"\n",
    "        negated_vector = TaskVector.__new__(TaskVector)\n",
    "        negated_vector.pretrained_model = self.pretrained_model\n",
    "        negated_vector.finetuned_model = self.finetuned_model\n",
    "        for param_pretrained, param_finetuned in zip(\n",
    "            negated_vector.pretrained_model.parameters(), negated_vector.finetuned_model.parameters()\n",
    "        ):\n",
    "            param_finetuned.data = param_pretrained.data - param_finetuned.data  # <----- play with\n",
    "        return negated_vector\n",
    "\n",
    "    def apply_to(self, base_model):\n",
    "        \"\"\"Applies the task vector to a base model's weights.\"\"\"\n",
    "        for param_base, param_pretrained, param_finetuned in zip(\n",
    "            base_model.parameters(), self.pretrained_model.parameters(), self.finetuned_model.parameters()\n",
    "        ):\n",
    "            param_base.data += scaling_coef * normalize_weights(param_finetuned.data - param_pretrained.data)  # <----- play with\n",
    "        return base_model\n",
    "\n",
    "# Initialize TaskVector\n",
    "task_vector = TaskVector(\"gpt2\", finetuned_model_path)  # <----- change\n",
    "\n",
    "# Negate the Task Vector to adjust toward negative sentiment\n",
    "neg_task_vector = -task_vector  # <----- play with\n",
    "\n",
    "# Sentence to transfer\n",
    "input_sentence = \"This is a simple sentence and not that long.\"  # <----- change\n",
    "\"\"\"\n",
    "prompt should look like this:\n",
    "[Instruction]: Provide a clear and concise task or question.\n",
    "[Context]: Include any necessary background information or examples.\n",
    "[Format]: Specify the desired response format (bullet points, paragraphs, JSON, etc.).\n",
    "[Constraints]: (Optional) Set any rules, word limits, or tone preferences.\n",
    "\"\"\"\n",
    "\n",
    "# Generate output using the fine-tuned model\n",
    "def generate_with_model(model, sentence):  # <----- play with\n",
    "    inputs = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "    output = model.generate(\n",
    "    inputs,\n",
    "    max_length=70,          # Limit output length\n",
    "    temperature=0.7,         # Adjust temperature for randomness\n",
    "    top_k=85,                # Limit sampling to top k candidates\n",
    "    top_p=0.9,               # Nucleus sampling for diversity\n",
    "    repetition_penalty=3.0,  # Apply repetition penalty\n",
    "    num_return_sequences=1   # Only generate one sequence\n",
    "    )\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Generate output after applying task vector\n",
    "base_model_copy = GPT2LMHeadModel.from_pretrained(\"gpt2\")  # <----- change\n",
    "scaling_coef = 0.8  # Adjust the scaling factor as needed\n",
    "task_adjusted_model = neg_task_vector.apply_to(base_model_copy)\n",
    "\n",
    "# Generate sentences\n",
    "finetuned_output = generate_with_model(finetuned_model, input_sentence)\n",
    "adjusted_output = generate_with_model(task_adjusted_model, input_sentence)\n",
    "\n",
    "# Print input and outputs\n",
    "print(\"Input Sentence:\", input_sentence)\n",
    "print(\"Output (Fine-Tuned Model):\", finetuned_output)\n",
    "print(\"Output (After Task Vector Adjustment):\", adjusted_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
